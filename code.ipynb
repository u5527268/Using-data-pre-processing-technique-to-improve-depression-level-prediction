{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference\n",
    "#title: early-stopping-pytorch. \n",
    "#author: Bjarte Mehus Sunde\n",
    "#address: https://github.com/Bjarten/early-stopping-pytorch\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "data_gsr = pd.read_csv('gsr.csv')\n",
    "data_pupil = pd.read_csv('pupil.csv')\n",
    "data_skintemp = pd.read_csv('skintemp.csv')\n",
    "data_combined = pd.read_csv('combined.csv')\n",
    "data_gsr = data_gsr.apply(pd.to_numeric)\n",
    "data_pupil = data_pupil.apply(pd.to_numeric)\n",
    "data_skintemp = data_skintemp.apply(pd.to_numeric)\n",
    "data_combined = data_combined.apply(pd.to_numeric)\n",
    "data_combined = abs(data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     depr_label  min_normalised_gsr  max_normalised_gsr  mean_normalised_gsr  \\\n",
      "0           2.0            0.716610            0.970557             0.821910   \n",
      "1           2.0            0.553351            0.718968             0.630662   \n",
      "2           2.0            0.530042            0.873545             0.607463   \n",
      "3           2.0            0.407362            0.515320             0.458619   \n",
      "4           0.0            0.307992            0.401229             0.353097   \n",
      "..          ...                 ...                 ...                  ...   \n",
      "187         2.0            0.478860            0.689492             0.531518   \n",
      "188         3.0            0.508756            0.824976             0.599765   \n",
      "189         3.0            0.574068            0.942852             0.709688   \n",
      "190         3.0            0.691630            0.969521             0.762679   \n",
      "191         3.0            0.713401            0.852734             0.757124   \n",
      "\n",
      "     std_normalised_gsr  var_normalised_gsr  rms_normalised_gsr  \\\n",
      "0              0.072007            0.005185            0.825042   \n",
      "1              0.047855            0.002290            0.632464   \n",
      "2              0.073371            0.005383            0.611853   \n",
      "3              0.030267            0.000916            0.459610   \n",
      "4              0.027189            0.000739            0.354137   \n",
      "..                  ...                 ...                 ...   \n",
      "187            0.050495            0.002550            0.533896   \n",
      "188            0.072180            0.005210            0.604066   \n",
      "189            0.087339            0.007628            0.715010   \n",
      "190            0.058613            0.003435            0.764914   \n",
      "191            0.034876            0.001216            0.757922   \n",
      "\n",
      "     min_filtered_gsr  max_filtered_gsr  mean_filtered_gsr  ...  \\\n",
      "0            0.980664          1.405398           1.093152  ...   \n",
      "1            0.803887          0.975663           0.888117  ...   \n",
      "2            0.775245          1.096903           0.851721  ...   \n",
      "3            0.652096          0.765979           0.703995  ...   \n",
      "4            0.543863          0.641232           0.591969  ...   \n",
      "..                ...               ...                ...  ...   \n",
      "187          3.405516          3.776510           3.493993  ...   \n",
      "188          3.469690          4.094220           3.680812  ...   \n",
      "189          3.590318          4.409810           3.910039  ...   \n",
      "190          3.831880          4.473302           4.055889  ...   \n",
      "191          3.956547          4.228696           4.052313  ...   \n",
      "\n",
      "     second_diff_normalised_skintemp_abs_mean  \\\n",
      "0                                    0.004944   \n",
      "1                                    0.005144   \n",
      "2                                    0.005573   \n",
      "3                                    0.004863   \n",
      "4                                    0.005788   \n",
      "..                                        ...   \n",
      "187                                  0.006883   \n",
      "188                                  0.007320   \n",
      "189                                  0.006555   \n",
      "190                                  0.006118   \n",
      "191                                  0.004484   \n",
      "\n",
      "     first_diff_filtered_skintemp_abs_mean  \\\n",
      "0                                 0.071831   \n",
      "1                                 0.001383   \n",
      "2                                 0.001172   \n",
      "3                                 0.001020   \n",
      "4                                 0.001512   \n",
      "..                                     ...   \n",
      "187                               0.001046   \n",
      "188                               0.001801   \n",
      "189                               0.001031   \n",
      "190                               0.001128   \n",
      "191                               0.001141   \n",
      "\n",
      "     second_diff_filtered_skintemp_abs_mean  num_normalised_skintemp_peaks  \\\n",
      "0                                  0.137465                       0.416667   \n",
      "1                                  0.002745                       0.333333   \n",
      "2                                  0.002322                       0.666667   \n",
      "3                                  0.002030                       0.666667   \n",
      "4                                  0.003005                       0.500000   \n",
      "..                                      ...                            ...   \n",
      "187                                0.002082                       0.416667   \n",
      "188                                0.003571                       0.500000   \n",
      "189                                0.002036                       0.500000   \n",
      "190                                0.002233                       0.500000   \n",
      "191                                0.002275                       0.333333   \n",
      "\n",
      "     num_filtered_skintemp_peaks  vlp_skintemp_peak_occurrences  \\\n",
      "0                            0.4                       0.333333   \n",
      "1                            0.3                       0.000000   \n",
      "2                            0.4                       0.333333   \n",
      "3                            0.2                       0.000000   \n",
      "4                            0.4                       0.333333   \n",
      "..                           ...                            ...   \n",
      "187                          0.3                       0.666667   \n",
      "188                          0.4                       0.666667   \n",
      "189                          0.4                       0.333333   \n",
      "190                          0.4                       0.333333   \n",
      "191                          0.2                       0.333333   \n",
      "\n",
      "     lp_skintemp_peak_occurrences  mean_vlp_skintemp_peak_amplitudes  \\\n",
      "0                        0.636364                           0.166276   \n",
      "1                        0.090909                           0.329021   \n",
      "2                        0.363636                           0.234310   \n",
      "3                        0.000000                           0.420307   \n",
      "4                        0.454545                           0.285511   \n",
      "..                            ...                                ...   \n",
      "187                      0.545455                           0.005077   \n",
      "188                      0.363636                           0.003171   \n",
      "189                      0.636364                           0.006459   \n",
      "190                      0.454545                           0.010085   \n",
      "191                      0.454545                           0.015378   \n",
      "\n",
      "     mean_lp_skintemp_peak_amplitudes  ratio_skintemp_peak_occurrence_vlp_lp  \n",
      "0                            0.002174                               0.200000  \n",
      "1                            0.064926                               0.250000  \n",
      "2                            0.013983                               0.285714  \n",
      "3                            0.010984                               0.333333  \n",
      "4                            0.008479                               0.250000  \n",
      "..                                ...                                    ...  \n",
      "187                          0.000736                               0.333333  \n",
      "188                          0.001051                               0.428571  \n",
      "189                          0.000129                               0.200000  \n",
      "190                          0.000088                               0.250000  \n",
      "191                          0.001840                               0.250000  \n",
      "\n",
      "[192 rows x 86 columns]\n"
     ]
    }
   ],
   "source": [
    "# print data\n",
    "print(data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\untitled\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_array = data_combined.as_matrix()\n",
    "split_train_test = np.random.rand(len(data_array))<0.8\n",
    "train_data = data_array[split_train_test]\n",
    "test_data = data_array[~split_train_test]\n",
    "x_array = train_data[:, 1:86]\n",
    "y_array = train_data[:, 0]\n",
    "print(x_array.shape)\n",
    "X = torch.tensor(x_array, dtype=torch.float)\n",
    "Y = torch.tensor(y_array, dtype=torch.long)\n",
    "x_array_test = test_data[:, 1:86]\n",
    "y_array_test = test_data[:, 0]\n",
    "X_test = torch.tensor(x_array_test, dtype=torch.float)\n",
    "Y_test = torch.tensor(y_array_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of neurons for input layer, hidden layer and output layer\n",
    "# define learning rate and number of epoch on training\n",
    "input_neurons = 85\n",
    "hidden_neurons = 50\n",
    "output_neurons = 4\n",
    "learning_rate = 0.01\n",
    "num_epoch = 500\n",
    "patience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_Networks(torch.nn.Module):\n",
    "    def __init__(self, num_input,num_hidden,num_output):\n",
    "        super(new_Networks,self).__init__()\n",
    "        self.hidden = torch.nn.Linear(num_input, num_hidden)\n",
    "        self.out = torch.nn.Linear(num_hidden, num_output)\n",
    "        torch.nn.Dropout(0.5)\n",
    "    def forward(self,x):\n",
    "        h_input = self.hidden(x)\n",
    "        h_output = torch.sigmoid(h_input)\n",
    "        y_pred = self.out(h_output)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = new_Networks(input_neurons,hidden_neurons,output_neurons)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "early_stoppings = EarlyStopping(patience=patience, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] Loss: 1.4003  Accuracy: 25.15 %\n",
      "Epoch [51/500] Loss: 1.1948  Accuracy: 44.17 %\n",
      "Epoch [101/500] Loss: 1.0676  Accuracy: 53.99 %\n",
      "Epoch [151/500] Loss: 0.9170  Accuracy: 60.12 %\n",
      "Epoch [201/500] Loss: 0.7548  Accuracy: 72.39 %\n",
      "Epoch [251/500] Loss: 0.6223  Accuracy: 82.21 %\n",
      "Epoch [301/500] Loss: 0.5058  Accuracy: 87.12 %\n",
      "Epoch [351/500] Loss: 0.4004  Accuracy: 90.80 %\n",
      "Epoch [401/500] Loss: 0.3166  Accuracy: 94.48 %\n",
      "Epoch [451/500] Loss: 0.2506  Accuracy: 96.93 %\n"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "valid_losses=[]\n",
    "for epoch in range(num_epoch):\n",
    "    Y_pred = net(X)\n",
    "    loss = loss_func(Y_pred, Y)\n",
    "    all_losses.append(loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        _, predicted = torch.max(F.softmax(Y_pred,1), 1)\n",
    "        total = predicted.size(0)\n",
    "        correct = predicted.data.numpy() == Y.data.numpy()\n",
    "        print('Epoch [%d/%d] Loss: %.4f  Accuracy: %.2f %%'\n",
    "              % (epoch + 1, num_epoch, loss.item(), 100 * sum(correct)/total))\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.725220 --> 1.720129).  Saving model ...\n",
      "Validation loss decreased (1.720129 --> 1.717598).  Saving model ...\n",
      "Validation loss decreased (1.717598 --> 1.715077).  Saving model ...\n",
      "Validation loss decreased (1.715077 --> 1.712565).  Saving model ...\n",
      "Validation loss decreased (1.712565 --> 1.710062).  Saving model ...\n",
      "Validation loss decreased (1.710062 --> 1.707569).  Saving model ...\n",
      "Validation loss decreased (1.707569 --> 1.705085).  Saving model ...\n",
      "Validation loss decreased (1.705085 --> 1.702610).  Saving model ...\n",
      "Validation loss decreased (1.702610 --> 1.700144).  Saving model ...\n",
      "Validation loss decreased (1.700144 --> 1.697688).  Saving model ...\n",
      "Validation loss decreased (1.697688 --> 1.695241).  Saving model ...\n",
      "Validation loss decreased (1.695241 --> 1.692802).  Saving model ...\n",
      "Validation loss decreased (1.692802 --> 1.690373).  Saving model ...\n",
      "Validation loss decreased (1.690373 --> 1.687953).  Saving model ...\n",
      "Validation loss decreased (1.687953 --> 1.685541).  Saving model ...\n",
      "Validation loss decreased (1.685541 --> 1.683138).  Saving model ...\n",
      "Validation loss decreased (1.683138 --> 1.680744).  Saving model ...\n",
      "Validation loss decreased (1.680744 --> 1.678359).  Saving model ...\n",
      "Validation loss decreased (1.678359 --> 1.675983).  Saving model ...\n",
      "Validation loss decreased (1.675983 --> 1.673615).  Saving model ...\n",
      "Validation loss decreased (1.673615 --> 1.671256).  Saving model ...\n",
      "Validation loss decreased (1.671256 --> 1.668905).  Saving model ...\n",
      "Validation loss decreased (1.668905 --> 1.666563).  Saving model ...\n",
      "Validation loss decreased (1.666563 --> 1.664229).  Saving model ...\n",
      "Validation loss decreased (1.664229 --> 1.661904).  Saving model ...\n",
      "Validation loss decreased (1.661904 --> 1.659587).  Saving model ...\n",
      "Validation loss decreased (1.659587 --> 1.657278).  Saving model ...\n",
      "Validation loss decreased (1.657278 --> 1.654978).  Saving model ...\n",
      "Validation loss decreased (1.654978 --> 1.652686).  Saving model ...\n",
      "Validation loss decreased (1.652686 --> 1.650402).  Saving model ...\n",
      "Validation loss decreased (1.650402 --> 1.648126).  Saving model ...\n",
      "Validation loss decreased (1.648126 --> 1.645858).  Saving model ...\n",
      "Validation loss decreased (1.645858 --> 1.643598).  Saving model ...\n",
      "Validation loss decreased (1.643598 --> 1.641347).  Saving model ...\n",
      "Validation loss decreased (1.641347 --> 1.639103).  Saving model ...\n",
      "Validation loss decreased (1.639103 --> 1.636867).  Saving model ...\n",
      "Validation loss decreased (1.636867 --> 1.634639).  Saving model ...\n",
      "Validation loss decreased (1.634639 --> 1.632419).  Saving model ...\n",
      "Validation loss decreased (1.632419 --> 1.630207).  Saving model ...\n",
      "Validation loss decreased (1.630207 --> 1.628003).  Saving model ...\n",
      "Validation loss decreased (1.628003 --> 1.625806).  Saving model ...\n",
      "Validation loss decreased (1.625806 --> 1.623617).  Saving model ...\n",
      "Validation loss decreased (1.623617 --> 1.621436).  Saving model ...\n",
      "Validation loss decreased (1.621436 --> 1.619262).  Saving model ...\n",
      "Validation loss decreased (1.619262 --> 1.617096).  Saving model ...\n",
      "Validation loss decreased (1.617096 --> 1.614937).  Saving model ...\n",
      "Validation loss decreased (1.614937 --> 1.612786).  Saving model ...\n",
      "Validation loss decreased (1.612786 --> 1.610642).  Saving model ...\n",
      "Validation loss decreased (1.610642 --> 1.608505).  Saving model ...\n",
      "Validation loss decreased (1.608505 --> 1.606376).  Saving model ...\n",
      "Validation loss decreased (1.606376 --> 1.604255).  Saving model ...\n",
      "Validation loss decreased (1.604255 --> 1.602140).  Saving model ...\n",
      "Validation loss decreased (1.602140 --> 1.600033).  Saving model ...\n",
      "Validation loss decreased (1.600033 --> 1.597933).  Saving model ...\n",
      "Validation loss decreased (1.597933 --> 1.595840).  Saving model ...\n",
      "Validation loss decreased (1.595840 --> 1.593755).  Saving model ...\n",
      "Validation loss decreased (1.593755 --> 1.591676).  Saving model ...\n",
      "Validation loss decreased (1.591676 --> 1.589605).  Saving model ...\n",
      "Validation loss decreased (1.589605 --> 1.587540).  Saving model ...\n",
      "Validation loss decreased (1.587540 --> 1.585483).  Saving model ...\n",
      "Validation loss decreased (1.585483 --> 1.583433).  Saving model ...\n",
      "Validation loss decreased (1.583433 --> 1.581389).  Saving model ...\n",
      "Validation loss decreased (1.581389 --> 1.579352).  Saving model ...\n",
      "Validation loss decreased (1.579352 --> 1.577323).  Saving model ...\n",
      "Validation loss decreased (1.577323 --> 1.575300).  Saving model ...\n",
      "Validation loss decreased (1.575300 --> 1.573283).  Saving model ...\n",
      "Validation loss decreased (1.573283 --> 1.571274).  Saving model ...\n",
      "Validation loss decreased (1.571274 --> 1.569271).  Saving model ...\n",
      "Validation loss decreased (1.569271 --> 1.567275).  Saving model ...\n",
      "Validation loss decreased (1.567275 --> 1.565286).  Saving model ...\n",
      "Validation loss decreased (1.565286 --> 1.563303).  Saving model ...\n",
      "Validation loss decreased (1.563303 --> 1.561327).  Saving model ...\n",
      "Validation loss decreased (1.561327 --> 1.559358).  Saving model ...\n",
      "Validation loss decreased (1.559358 --> 1.557395).  Saving model ...\n",
      "Validation loss decreased (1.557395 --> 1.555438).  Saving model ...\n",
      "Validation loss decreased (1.555438 --> 1.553488).  Saving model ...\n",
      "Validation loss decreased (1.553488 --> 1.551544).  Saving model ...\n",
      "Validation loss decreased (1.551544 --> 1.549607).  Saving model ...\n",
      "Validation loss decreased (1.549607 --> 1.547676).  Saving model ...\n",
      "Validation loss decreased (1.547676 --> 1.545751).  Saving model ...\n",
      "Validation loss decreased (1.545751 --> 1.543833).  Saving model ...\n",
      "Validation loss decreased (1.543833 --> 1.541921).  Saving model ...\n",
      "Validation loss decreased (1.541921 --> 1.540015).  Saving model ...\n",
      "Validation loss decreased (1.540015 --> 1.538116).  Saving model ...\n",
      "Validation loss decreased (1.538116 --> 1.536222).  Saving model ...\n",
      "Validation loss decreased (1.536222 --> 1.534335).  Saving model ...\n",
      "Validation loss decreased (1.534335 --> 1.532454).  Saving model ...\n",
      "Validation loss decreased (1.532454 --> 1.530579).  Saving model ...\n",
      "Validation loss decreased (1.530579 --> 1.528710).  Saving model ...\n",
      "Validation loss decreased (1.528710 --> 1.526847).  Saving model ...\n",
      "Validation loss decreased (1.526847 --> 1.524990).  Saving model ...\n",
      "Validation loss decreased (1.524990 --> 1.523139).  Saving model ...\n",
      "Validation loss decreased (1.523139 --> 1.521294).  Saving model ...\n",
      "Validation loss decreased (1.521294 --> 1.519455).  Saving model ...\n",
      "Validation loss decreased (1.519455 --> 1.517621).  Saving model ...\n",
      "Validation loss decreased (1.517621 --> 1.515794).  Saving model ...\n",
      "Validation loss decreased (1.515794 --> 1.513972).  Saving model ...\n",
      "Validation loss decreased (1.513972 --> 1.512157).  Saving model ...\n",
      "Validation loss decreased (1.512157 --> 1.510347).  Saving model ...\n",
      "Validation loss decreased (1.510347 --> 1.508542).  Saving model ...\n",
      "Validation loss decreased (1.508542 --> 1.506744).  Saving model ...\n",
      "Validation loss decreased (1.506744 --> 1.504951).  Saving model ...\n",
      "Validation loss decreased (1.504951 --> 1.503164).  Saving model ...\n",
      "Validation loss decreased (1.503164 --> 1.501382).  Saving model ...\n",
      "Validation loss decreased (1.501382 --> 1.499606).  Saving model ...\n",
      "Validation loss decreased (1.499606 --> 1.497836).  Saving model ...\n",
      "Validation loss decreased (1.497836 --> 1.496071).  Saving model ...\n",
      "Validation loss decreased (1.496071 --> 1.494312).  Saving model ...\n",
      "Validation loss decreased (1.494312 --> 1.492558).  Saving model ...\n",
      "Validation loss decreased (1.492558 --> 1.490810).  Saving model ...\n",
      "Validation loss decreased (1.490810 --> 1.489067).  Saving model ...\n",
      "Validation loss decreased (1.489067 --> 1.487330).  Saving model ...\n",
      "Validation loss decreased (1.487330 --> 1.485598).  Saving model ...\n",
      "Validation loss decreased (1.485598 --> 1.483871).  Saving model ...\n",
      "Validation loss decreased (1.483871 --> 1.482150).  Saving model ...\n",
      "Validation loss decreased (1.482150 --> 1.480434).  Saving model ...\n",
      "Validation loss decreased (1.480434 --> 1.478723).  Saving model ...\n",
      "Validation loss decreased (1.478723 --> 1.477018).  Saving model ...\n",
      "Validation loss decreased (1.477018 --> 1.475318).  Saving model ...\n",
      "Validation loss decreased (1.475318 --> 1.473623).  Saving model ...\n",
      "Validation loss decreased (1.473623 --> 1.471934).  Saving model ...\n",
      "Validation loss decreased (1.471934 --> 1.470249).  Saving model ...\n",
      "Validation loss decreased (1.470249 --> 1.468570).  Saving model ...\n",
      "Validation loss decreased (1.468570 --> 1.466896).  Saving model ...\n",
      "Validation loss decreased (1.466896 --> 1.465227).  Saving model ...\n",
      "Validation loss decreased (1.465227 --> 1.463563).  Saving model ...\n",
      "Validation loss decreased (1.463563 --> 1.461904).  Saving model ...\n",
      "Validation loss decreased (1.461904 --> 1.460250).  Saving model ...\n",
      "Validation loss decreased (1.460250 --> 1.458601).  Saving model ...\n",
      "Validation loss decreased (1.458601 --> 1.456957).  Saving model ...\n",
      "Validation loss decreased (1.456957 --> 1.455318).  Saving model ...\n",
      "Validation loss decreased (1.455318 --> 1.453684).  Saving model ...\n",
      "Validation loss decreased (1.453684 --> 1.452055).  Saving model ...\n",
      "Validation loss decreased (1.452055 --> 1.450431).  Saving model ...\n",
      "Validation loss decreased (1.450431 --> 1.448812).  Saving model ...\n",
      "Validation loss decreased (1.448812 --> 1.447198).  Saving model ...\n",
      "Validation loss decreased (1.447198 --> 1.445588).  Saving model ...\n",
      "Validation loss decreased (1.445588 --> 1.443984).  Saving model ...\n",
      "Validation loss decreased (1.443984 --> 1.442384).  Saving model ...\n",
      "Validation loss decreased (1.442384 --> 1.440789).  Saving model ...\n",
      "Validation loss decreased (1.440789 --> 1.439198).  Saving model ...\n",
      "Validation loss decreased (1.439198 --> 1.437613).  Saving model ...\n",
      "Validation loss decreased (1.437613 --> 1.436032).  Saving model ...\n",
      "Validation loss decreased (1.436032 --> 1.434456).  Saving model ...\n",
      "Validation loss decreased (1.434456 --> 1.432884).  Saving model ...\n",
      "Validation loss decreased (1.432884 --> 1.431317).  Saving model ...\n",
      "Validation loss decreased (1.431317 --> 1.429755).  Saving model ...\n",
      "Validation loss decreased (1.429755 --> 1.428197).  Saving model ...\n",
      "Validation loss decreased (1.428197 --> 1.426644).  Saving model ...\n",
      "Validation loss decreased (1.426644 --> 1.425096).  Saving model ...\n",
      "Validation loss decreased (1.425096 --> 1.423552).  Saving model ...\n",
      "Validation loss decreased (1.423552 --> 1.422012).  Saving model ...\n",
      "Validation loss decreased (1.422012 --> 1.420477).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.420477 --> 1.418947).  Saving model ...\n",
      "Validation loss decreased (1.418947 --> 1.417421).  Saving model ...\n",
      "Validation loss decreased (1.417421 --> 1.415900).  Saving model ...\n",
      "Validation loss decreased (1.415900 --> 1.414382).  Saving model ...\n",
      "Validation loss decreased (1.414382 --> 1.412870).  Saving model ...\n",
      "Validation loss decreased (1.412870 --> 1.411362).  Saving model ...\n",
      "Validation loss decreased (1.411362 --> 1.409858).  Saving model ...\n",
      "Validation loss decreased (1.409858 --> 1.408358).  Saving model ...\n",
      "Validation loss decreased (1.408358 --> 1.406863).  Saving model ...\n",
      "Validation loss decreased (1.406863 --> 1.405372).  Saving model ...\n",
      "Validation loss decreased (1.405372 --> 1.403885).  Saving model ...\n",
      "Validation loss decreased (1.403885 --> 1.402403).  Saving model ...\n",
      "Validation loss decreased (1.402403 --> 1.400925).  Saving model ...\n",
      "Validation loss decreased (1.400925 --> 1.399451).  Saving model ...\n",
      "Validation loss decreased (1.399451 --> 1.397982).  Saving model ...\n",
      "Validation loss decreased (1.397982 --> 1.396516).  Saving model ...\n",
      "Validation loss decreased (1.396516 --> 1.395055).  Saving model ...\n",
      "Validation loss decreased (1.395055 --> 1.393598).  Saving model ...\n",
      "Validation loss decreased (1.393598 --> 1.392145).  Saving model ...\n",
      "Validation loss decreased (1.392145 --> 1.390696).  Saving model ...\n",
      "Validation loss decreased (1.390696 --> 1.389252).  Saving model ...\n",
      "Validation loss decreased (1.389252 --> 1.387811).  Saving model ...\n",
      "Validation loss decreased (1.387811 --> 1.386375).  Saving model ...\n",
      "Validation loss decreased (1.386375 --> 1.384942).  Saving model ...\n",
      "Validation loss decreased (1.384942 --> 1.383514).  Saving model ...\n",
      "Validation loss decreased (1.383514 --> 1.382090).  Saving model ...\n",
      "Validation loss decreased (1.382090 --> 1.380669).  Saving model ...\n",
      "Validation loss decreased (1.380669 --> 1.379253).  Saving model ...\n",
      "Validation loss decreased (1.379253 --> 1.377841).  Saving model ...\n",
      "Validation loss decreased (1.377841 --> 1.376432).  Saving model ...\n",
      "Validation loss decreased (1.376432 --> 1.375028).  Saving model ...\n",
      "Validation loss decreased (1.375028 --> 1.373627).  Saving model ...\n",
      "Validation loss decreased (1.373627 --> 1.372231).  Saving model ...\n",
      "Validation loss decreased (1.372231 --> 1.370838).  Saving model ...\n",
      "Validation loss decreased (1.370838 --> 1.369449).  Saving model ...\n",
      "Validation loss decreased (1.369449 --> 1.368064).  Saving model ...\n",
      "Validation loss decreased (1.368064 --> 1.366683).  Saving model ...\n",
      "Validation loss decreased (1.366683 --> 1.365306).  Saving model ...\n",
      "Validation loss decreased (1.365306 --> 1.363932).  Saving model ...\n",
      "Validation loss decreased (1.363932 --> 1.362563).  Saving model ...\n",
      "Validation loss decreased (1.362563 --> 1.361197).  Saving model ...\n",
      "Validation loss decreased (1.361197 --> 1.359835).  Saving model ...\n",
      "Validation loss decreased (1.359835 --> 1.358476).  Saving model ...\n",
      "Validation loss decreased (1.358476 --> 1.357122).  Saving model ...\n",
      "Validation loss decreased (1.357122 --> 1.355771).  Saving model ...\n",
      "Validation loss decreased (1.355771 --> 1.354424).  Saving model ...\n",
      "Validation loss decreased (1.354424 --> 1.353080).  Saving model ...\n",
      "Validation loss decreased (1.353080 --> 1.351740).  Saving model ...\n",
      "Validation loss decreased (1.351740 --> 1.350404).  Saving model ...\n",
      "Validation loss decreased (1.350404 --> 1.349072).  Saving model ...\n",
      "Validation loss decreased (1.349072 --> 1.347743).  Saving model ...\n",
      "Validation loss decreased (1.347743 --> 1.346417).  Saving model ...\n",
      "Validation loss decreased (1.346417 --> 1.345096).  Saving model ...\n",
      "Validation loss decreased (1.345096 --> 1.343778).  Saving model ...\n",
      "Validation loss decreased (1.343778 --> 1.342463).  Saving model ...\n",
      "Validation loss decreased (1.342463 --> 1.341152).  Saving model ...\n",
      "Validation loss decreased (1.341152 --> 1.339845).  Saving model ...\n",
      "Validation loss decreased (1.339845 --> 1.338541).  Saving model ...\n",
      "Validation loss decreased (1.338541 --> 1.337241).  Saving model ...\n",
      "Validation loss decreased (1.337241 --> 1.335944).  Saving model ...\n",
      "Validation loss decreased (1.335944 --> 1.334650).  Saving model ...\n",
      "Validation loss decreased (1.334650 --> 1.333360).  Saving model ...\n",
      "Validation loss decreased (1.333360 --> 1.332074).  Saving model ...\n",
      "Validation loss decreased (1.332074 --> 1.330791).  Saving model ...\n",
      "Validation loss decreased (1.330791 --> 1.329511).  Saving model ...\n",
      "Validation loss decreased (1.329511 --> 1.328235).  Saving model ...\n",
      "Validation loss decreased (1.328235 --> 1.326963).  Saving model ...\n",
      "Validation loss decreased (1.326963 --> 1.325693).  Saving model ...\n",
      "Validation loss decreased (1.325693 --> 1.324427).  Saving model ...\n",
      "Validation loss decreased (1.324427 --> 1.323165).  Saving model ...\n",
      "Validation loss decreased (1.323165 --> 1.321905).  Saving model ...\n",
      "Validation loss decreased (1.321905 --> 1.320649).  Saving model ...\n",
      "Validation loss decreased (1.320649 --> 1.319397).  Saving model ...\n",
      "Validation loss decreased (1.319397 --> 1.318147).  Saving model ...\n",
      "Validation loss decreased (1.318147 --> 1.316901).  Saving model ...\n",
      "Validation loss decreased (1.316901 --> 1.315659).  Saving model ...\n",
      "Validation loss decreased (1.315659 --> 1.314419).  Saving model ...\n",
      "Validation loss decreased (1.314419 --> 1.313183).  Saving model ...\n",
      "Validation loss decreased (1.313183 --> 1.311950).  Saving model ...\n",
      "Validation loss decreased (1.311950 --> 1.310720).  Saving model ...\n",
      "Validation loss decreased (1.310720 --> 1.309494).  Saving model ...\n",
      "Validation loss decreased (1.309494 --> 1.308270).  Saving model ...\n",
      "Validation loss decreased (1.308270 --> 1.307050).  Saving model ...\n",
      "Validation loss decreased (1.307050 --> 1.305833).  Saving model ...\n",
      "Validation loss decreased (1.305833 --> 1.304620).  Saving model ...\n",
      "Validation loss decreased (1.304620 --> 1.303409).  Saving model ...\n",
      "Validation loss decreased (1.303409 --> 1.302202).  Saving model ...\n",
      "Validation loss decreased (1.302202 --> 1.300997).  Saving model ...\n",
      "Validation loss decreased (1.300997 --> 1.299796).  Saving model ...\n",
      "Validation loss decreased (1.299796 --> 1.298598).  Saving model ...\n",
      "Validation loss decreased (1.298598 --> 1.297403).  Saving model ...\n",
      "Validation loss decreased (1.297403 --> 1.296211).  Saving model ...\n",
      "Validation loss decreased (1.296211 --> 1.295022).  Saving model ...\n",
      "Validation loss decreased (1.295022 --> 1.293836).  Saving model ...\n",
      "Validation loss decreased (1.293836 --> 1.292654).  Saving model ...\n",
      "Validation loss decreased (1.292654 --> 1.291474).  Saving model ...\n",
      "Validation loss decreased (1.291474 --> 1.290297).  Saving model ...\n",
      "Validation loss decreased (1.290297 --> 1.289124).  Saving model ...\n",
      "Validation loss decreased (1.289124 --> 1.287953).  Saving model ...\n",
      "Validation loss decreased (1.287953 --> 1.286785).  Saving model ...\n",
      "Validation loss decreased (1.286785 --> 1.285621).  Saving model ...\n",
      "Validation loss decreased (1.285621 --> 1.284459).  Saving model ...\n",
      "Validation loss decreased (1.284459 --> 1.283300).  Saving model ...\n",
      "Validation loss decreased (1.283300 --> 1.282144).  Saving model ...\n",
      "Validation loss decreased (1.282144 --> 1.280991).  Saving model ...\n",
      "Validation loss decreased (1.280991 --> 1.279841).  Saving model ...\n",
      "Validation loss decreased (1.279841 --> 1.278694).  Saving model ...\n",
      "Validation loss decreased (1.278694 --> 1.277550).  Saving model ...\n",
      "Validation loss decreased (1.277550 --> 1.276409).  Saving model ...\n",
      "Validation loss decreased (1.276409 --> 1.275271).  Saving model ...\n",
      "Validation loss decreased (1.275271 --> 1.274135).  Saving model ...\n",
      "Validation loss decreased (1.274135 --> 1.273002).  Saving model ...\n",
      "Validation loss decreased (1.273002 --> 1.271873).  Saving model ...\n",
      "Validation loss decreased (1.271873 --> 1.270746).  Saving model ...\n",
      "Validation loss decreased (1.270746 --> 1.269621).  Saving model ...\n",
      "Validation loss decreased (1.269621 --> 1.268500).  Saving model ...\n",
      "Validation loss decreased (1.268500 --> 1.267382).  Saving model ...\n",
      "Validation loss decreased (1.267382 --> 1.266266).  Saving model ...\n",
      "Validation loss decreased (1.266266 --> 1.265153).  Saving model ...\n",
      "Validation loss decreased (1.265153 --> 1.264043).  Saving model ...\n",
      "Validation loss decreased (1.264043 --> 1.262935).  Saving model ...\n",
      "Validation loss decreased (1.262935 --> 1.261831).  Saving model ...\n",
      "Validation loss decreased (1.261831 --> 1.260729).  Saving model ...\n",
      "Validation loss decreased (1.260729 --> 1.259629).  Saving model ...\n",
      "Validation loss decreased (1.259629 --> 1.258533).  Saving model ...\n",
      "Validation loss decreased (1.258533 --> 1.257439).  Saving model ...\n",
      "Validation loss decreased (1.257439 --> 1.256348).  Saving model ...\n",
      "Validation loss decreased (1.256348 --> 1.255260).  Saving model ...\n",
      "Validation loss decreased (1.255260 --> 1.254174).  Saving model ...\n",
      "Validation loss decreased (1.254174 --> 1.253091).  Saving model ...\n",
      "Validation loss decreased (1.253091 --> 1.252011).  Saving model ...\n",
      "Validation loss decreased (1.252011 --> 1.250933).  Saving model ...\n",
      "Validation loss decreased (1.250933 --> 1.249858).  Saving model ...\n",
      "Validation loss decreased (1.249858 --> 1.248785).  Saving model ...\n",
      "Validation loss decreased (1.248785 --> 1.247716).  Saving model ...\n",
      "Validation loss decreased (1.247716 --> 1.246648).  Saving model ...\n",
      "Validation loss decreased (1.246648 --> 1.245584).  Saving model ...\n",
      "Validation loss decreased (1.245584 --> 1.244522).  Saving model ...\n",
      "Validation loss decreased (1.244522 --> 1.243462).  Saving model ...\n",
      "Validation loss decreased (1.243462 --> 1.242405).  Saving model ...\n",
      "Validation loss decreased (1.242405 --> 1.241351).  Saving model ...\n",
      "Validation loss decreased (1.241351 --> 1.240299).  Saving model ...\n",
      "Validation loss decreased (1.240299 --> 1.239250).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.239250 --> 1.238204).  Saving model ...\n",
      "Validation loss decreased (1.238204 --> 1.237159).  Saving model ...\n",
      "Validation loss decreased (1.237159 --> 1.236118).  Saving model ...\n",
      "Validation loss decreased (1.236118 --> 1.235079).  Saving model ...\n",
      "Validation loss decreased (1.235079 --> 1.234042).  Saving model ...\n",
      "Validation loss decreased (1.234042 --> 1.233008).  Saving model ...\n",
      "Validation loss decreased (1.233008 --> 1.231976).  Saving model ...\n",
      "Validation loss decreased (1.231976 --> 1.230947).  Saving model ...\n",
      "Validation loss decreased (1.230947 --> 1.229921).  Saving model ...\n",
      "Validation loss decreased (1.229921 --> 1.228896).  Saving model ...\n",
      "Validation loss decreased (1.228896 --> 1.227875).  Saving model ...\n",
      "Validation loss decreased (1.227875 --> 1.226855).  Saving model ...\n",
      "Validation loss decreased (1.226855 --> 1.225838).  Saving model ...\n",
      "Validation loss decreased (1.225838 --> 1.224824).  Saving model ...\n",
      "Validation loss decreased (1.224824 --> 1.223812).  Saving model ...\n",
      "Validation loss decreased (1.223812 --> 1.222802).  Saving model ...\n",
      "Validation loss decreased (1.222802 --> 1.221795).  Saving model ...\n",
      "Validation loss decreased (1.221795 --> 1.220790).  Saving model ...\n",
      "Validation loss decreased (1.220790 --> 1.219787).  Saving model ...\n",
      "Validation loss decreased (1.219787 --> 1.218787).  Saving model ...\n",
      "Validation loss decreased (1.218787 --> 1.217789).  Saving model ...\n",
      "Validation loss decreased (1.217789 --> 1.216794).  Saving model ...\n",
      "Validation loss decreased (1.216794 --> 1.215801).  Saving model ...\n",
      "Validation loss decreased (1.215801 --> 1.214810).  Saving model ...\n",
      "Validation loss decreased (1.214810 --> 1.213822).  Saving model ...\n",
      "Validation loss decreased (1.213822 --> 1.212836).  Saving model ...\n",
      "Validation loss decreased (1.212836 --> 1.211852).  Saving model ...\n",
      "Validation loss decreased (1.211852 --> 1.210870).  Saving model ...\n",
      "Validation loss decreased (1.210870 --> 1.209891).  Saving model ...\n",
      "Validation loss decreased (1.209891 --> 1.208914).  Saving model ...\n",
      "Validation loss decreased (1.208914 --> 1.207940).  Saving model ...\n",
      "Validation loss decreased (1.207940 --> 1.206967).  Saving model ...\n",
      "Validation loss decreased (1.206967 --> 1.205997).  Saving model ...\n",
      "Validation loss decreased (1.205997 --> 1.205030).  Saving model ...\n",
      "Validation loss decreased (1.205030 --> 1.204064).  Saving model ...\n",
      "Validation loss decreased (1.204064 --> 1.203101).  Saving model ...\n",
      "Validation loss decreased (1.203101 --> 1.202140).  Saving model ...\n",
      "Validation loss decreased (1.202140 --> 1.201181).  Saving model ...\n",
      "Validation loss decreased (1.201181 --> 1.200224).  Saving model ...\n",
      "Validation loss decreased (1.200224 --> 1.199270).  Saving model ...\n",
      "Validation loss decreased (1.199270 --> 1.198317).  Saving model ...\n",
      "Validation loss decreased (1.198317 --> 1.197367).  Saving model ...\n",
      "Validation loss decreased (1.197367 --> 1.196419).  Saving model ...\n",
      "Validation loss decreased (1.196419 --> 1.195474).  Saving model ...\n",
      "Validation loss decreased (1.195474 --> 1.194530).  Saving model ...\n",
      "Validation loss decreased (1.194530 --> 1.193589).  Saving model ...\n",
      "Validation loss decreased (1.193589 --> 1.192650).  Saving model ...\n",
      "Validation loss decreased (1.192650 --> 1.191713).  Saving model ...\n",
      "Validation loss decreased (1.191713 --> 1.190778).  Saving model ...\n",
      "Validation loss decreased (1.190778 --> 1.189845).  Saving model ...\n",
      "Validation loss decreased (1.189845 --> 1.188914).  Saving model ...\n",
      "Validation loss decreased (1.188914 --> 1.187986).  Saving model ...\n",
      "Validation loss decreased (1.187986 --> 1.187059).  Saving model ...\n",
      "Validation loss decreased (1.187059 --> 1.186135).  Saving model ...\n",
      "Validation loss decreased (1.186135 --> 1.185213).  Saving model ...\n",
      "Validation loss decreased (1.185213 --> 1.184293).  Saving model ...\n",
      "Validation loss decreased (1.184293 --> 1.183375).  Saving model ...\n",
      "Validation loss decreased (1.183375 --> 1.182459).  Saving model ...\n",
      "Validation loss decreased (1.182459 --> 1.181545).  Saving model ...\n",
      "Validation loss decreased (1.181545 --> 1.180633).  Saving model ...\n",
      "Validation loss decreased (1.180633 --> 1.179723).  Saving model ...\n",
      "Validation loss decreased (1.179723 --> 1.178815).  Saving model ...\n",
      "Validation loss decreased (1.178815 --> 1.177910).  Saving model ...\n",
      "Validation loss decreased (1.177910 --> 1.177006).  Saving model ...\n",
      "Validation loss decreased (1.177006 --> 1.176104).  Saving model ...\n",
      "Validation loss decreased (1.176104 --> 1.175205).  Saving model ...\n",
      "Validation loss decreased (1.175205 --> 1.174307).  Saving model ...\n",
      "Validation loss decreased (1.174307 --> 1.173412).  Saving model ...\n",
      "Validation loss decreased (1.173412 --> 1.172518).  Saving model ...\n",
      "Validation loss decreased (1.172518 --> 1.171627).  Saving model ...\n",
      "Validation loss decreased (1.171627 --> 1.170737).  Saving model ...\n",
      "Validation loss decreased (1.170737 --> 1.169849).  Saving model ...\n",
      "Validation loss decreased (1.169849 --> 1.168964).  Saving model ...\n",
      "Validation loss decreased (1.168964 --> 1.168080).  Saving model ...\n",
      "Validation loss decreased (1.168080 --> 1.167198).  Saving model ...\n",
      "Validation loss decreased (1.167198 --> 1.166319).  Saving model ...\n",
      "Validation loss decreased (1.166319 --> 1.165441).  Saving model ...\n",
      "Validation loss decreased (1.165441 --> 1.164565).  Saving model ...\n",
      "Validation loss decreased (1.164565 --> 1.163691).  Saving model ...\n",
      "Validation loss decreased (1.163691 --> 1.162819).  Saving model ...\n",
      "Validation loss decreased (1.162819 --> 1.161949).  Saving model ...\n",
      "Validation loss decreased (1.161949 --> 1.161081).  Saving model ...\n",
      "Validation loss decreased (1.161081 --> 1.160215).  Saving model ...\n",
      "Validation loss decreased (1.160215 --> 1.159350).  Saving model ...\n",
      "Validation loss decreased (1.159350 --> 1.158488).  Saving model ...\n",
      "Validation loss decreased (1.158488 --> 1.157627).  Saving model ...\n",
      "Validation loss decreased (1.157627 --> 1.156769).  Saving model ...\n",
      "Validation loss decreased (1.156769 --> 1.155912).  Saving model ...\n",
      "Validation loss decreased (1.155912 --> 1.155057).  Saving model ...\n",
      "Validation loss decreased (1.155057 --> 1.154204).  Saving model ...\n",
      "Validation loss decreased (1.154204 --> 1.153353).  Saving model ...\n",
      "Validation loss decreased (1.153353 --> 1.152503).  Saving model ...\n",
      "Validation loss decreased (1.152503 --> 1.151656).  Saving model ...\n",
      "Validation loss decreased (1.151656 --> 1.150810).  Saving model ...\n",
      "Validation loss decreased (1.150810 --> 1.149967).  Saving model ...\n",
      "Validation loss decreased (1.149967 --> 1.149125).  Saving model ...\n",
      "Validation loss decreased (1.149125 --> 1.148284).  Saving model ...\n",
      "Validation loss decreased (1.148284 --> 1.147446).  Saving model ...\n",
      "Validation loss decreased (1.147446 --> 1.146610).  Saving model ...\n",
      "Validation loss decreased (1.146610 --> 1.145775).  Saving model ...\n",
      "Validation loss decreased (1.145775 --> 1.144942).  Saving model ...\n",
      "Validation loss decreased (1.144942 --> 1.144111).  Saving model ...\n",
      "Validation loss decreased (1.144111 --> 1.143281).  Saving model ...\n",
      "Validation loss decreased (1.143281 --> 1.142454).  Saving model ...\n",
      "Validation loss decreased (1.142454 --> 1.141628).  Saving model ...\n",
      "Validation loss decreased (1.141628 --> 1.140804).  Saving model ...\n",
      "Validation loss decreased (1.140804 --> 1.139982).  Saving model ...\n",
      "Validation loss decreased (1.139982 --> 1.139161).  Saving model ...\n",
      "Validation loss decreased (1.139161 --> 1.138343).  Saving model ...\n",
      "Validation loss decreased (1.138343 --> 1.137526).  Saving model ...\n",
      "Validation loss decreased (1.137526 --> 1.136711).  Saving model ...\n",
      "Validation loss decreased (1.136711 --> 1.135897).  Saving model ...\n",
      "Validation loss decreased (1.135897 --> 1.135085).  Saving model ...\n",
      "Validation loss decreased (1.135085 --> 1.134275).  Saving model ...\n",
      "Validation loss decreased (1.134275 --> 1.133467).  Saving model ...\n",
      "Validation loss decreased (1.133467 --> 1.132660).  Saving model ...\n",
      "Validation loss decreased (1.132660 --> 1.131856).  Saving model ...\n",
      "Validation loss decreased (1.131856 --> 1.131052).  Saving model ...\n",
      "Validation loss decreased (1.131052 --> 1.130251).  Saving model ...\n",
      "Validation loss decreased (1.130251 --> 1.129451).  Saving model ...\n",
      "Validation loss decreased (1.129451 --> 1.128653).  Saving model ...\n",
      "Validation loss decreased (1.128653 --> 1.127857).  Saving model ...\n",
      "Validation loss decreased (1.127857 --> 1.127062).  Saving model ...\n",
      "Validation loss decreased (1.127062 --> 1.126269).  Saving model ...\n",
      "Validation loss decreased (1.126269 --> 1.125478).  Saving model ...\n",
      "Validation loss decreased (1.125478 --> 1.124688).  Saving model ...\n",
      "Validation loss decreased (1.124688 --> 1.123900).  Saving model ...\n",
      "Validation loss decreased (1.123900 --> 1.123113).  Saving model ...\n",
      "Validation loss decreased (1.123113 --> 1.122329).  Saving model ...\n",
      "Validation loss decreased (1.122329 --> 1.121546).  Saving model ...\n",
      "Validation loss decreased (1.121546 --> 1.120764).  Saving model ...\n",
      "Validation loss decreased (1.120764 --> 1.119984).  Saving model ...\n",
      "Validation loss decreased (1.119984 --> 1.119206).  Saving model ...\n",
      "Validation loss decreased (1.119206 --> 1.118429).  Saving model ...\n",
      "Validation loss decreased (1.118429 --> 1.117654).  Saving model ...\n",
      "Validation loss decreased (1.117654 --> 1.116881).  Saving model ...\n",
      "Validation loss decreased (1.116881 --> 1.116109).  Saving model ...\n",
      "Validation loss decreased (1.116109 --> 1.115339).  Saving model ...\n",
      "Validation loss decreased (1.115339 --> 1.114571).  Saving model ...\n",
      "Validation loss decreased (1.114571 --> 1.113804).  Saving model ...\n",
      "Validation loss decreased (1.113804 --> 1.113038).  Saving model ...\n",
      "Validation loss decreased (1.113038 --> 1.112274).  Saving model ...\n",
      "Validation loss decreased (1.112274 --> 1.111512).  Saving model ...\n",
      "Validation loss decreased (1.111512 --> 1.110751).  Saving model ...\n",
      "Validation loss decreased (1.110751 --> 1.109992).  Saving model ...\n",
      "Validation loss decreased (1.109992 --> 1.109235).  Saving model ...\n",
      "Validation loss decreased (1.109235 --> 1.108479).  Saving model ...\n",
      "Validation loss decreased (1.108479 --> 1.107724).  Saving model ...\n",
      "Validation loss decreased (1.107724 --> 1.106972).  Saving model ...\n",
      "Validation loss decreased (1.106972 --> 1.106220).  Saving model ...\n",
      "Validation loss decreased (1.106220 --> 1.105470).  Saving model ...\n",
      "Validation loss decreased (1.105470 --> 1.104722).  Saving model ...\n",
      "Validation loss decreased (1.104722 --> 1.103975).  Saving model ...\n",
      "Validation loss decreased (1.103975 --> 1.103230).  Saving model ...\n",
      "Validation loss decreased (1.103230 --> 1.102486).  Saving model ...\n",
      "Validation loss decreased (1.102486 --> 1.101744).  Saving model ...\n",
      "Validation loss decreased (1.101744 --> 1.101004).  Saving model ...\n",
      "Validation loss decreased (1.101004 --> 1.100264).  Saving model ...\n",
      "Validation loss decreased (1.100264 --> 1.099527).  Saving model ...\n",
      "Validation loss decreased (1.099527 --> 1.098791).  Saving model ...\n",
      "Validation loss decreased (1.098791 --> 1.098056).  Saving model ...\n",
      "Validation loss decreased (1.098056 --> 1.097323).  Saving model ...\n",
      "Validation loss decreased (1.097323 --> 1.096591).  Saving model ...\n",
      "Validation loss decreased (1.096591 --> 1.095861).  Saving model ...\n",
      "Validation loss decreased (1.095861 --> 1.095132).  Saving model ...\n",
      "Validation loss decreased (1.095132 --> 1.094405).  Saving model ...\n",
      "Validation loss decreased (1.094405 --> 1.093679).  Saving model ...\n",
      "Validation loss decreased (1.093679 --> 1.092955).  Saving model ...\n",
      "Validation loss decreased (1.092955 --> 1.092232).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.092232 --> 1.091510).  Saving model ...\n",
      "Validation loss decreased (1.091510 --> 1.090790).  Saving model ...\n",
      "Validation loss decreased (1.090790 --> 1.090072).  Saving model ...\n",
      "Validation loss decreased (1.090072 --> 1.089355).  Saving model ...\n",
      "Validation loss decreased (1.089355 --> 1.088639).  Saving model ...\n",
      "Validation loss decreased (1.088639 --> 1.087925).  Saving model ...\n",
      "Validation loss decreased (1.087925 --> 1.087212).  Saving model ...\n",
      "Validation loss decreased (1.087212 --> 1.086500).  Saving model ...\n",
      "Validation loss decreased (1.086500 --> 1.085790).  Saving model ...\n",
      "Validation loss decreased (1.085790 --> 1.085082).  Saving model ...\n",
      "Validation loss decreased (1.085082 --> 1.084375).  Saving model ...\n",
      "Validation loss decreased (1.084375 --> 1.083669).  Saving model ...\n",
      "Validation loss decreased (1.083669 --> 1.082965).  Saving model ...\n",
      "Validation loss decreased (1.082965 --> 1.082262).  Saving model ...\n",
      "Validation loss decreased (1.082262 --> 1.081560).  Saving model ...\n",
      "Validation loss decreased (1.081560 --> 1.080860).  Saving model ...\n",
      "Validation loss decreased (1.080860 --> 1.080161).  Saving model ...\n",
      "Validation loss decreased (1.080161 --> 1.079464).  Saving model ...\n",
      "Validation loss decreased (1.079464 --> 1.078768).  Saving model ...\n",
      "Validation loss decreased (1.078768 --> 1.078073).  Saving model ...\n",
      "Validation loss decreased (1.078073 --> 1.077380).  Saving model ...\n",
      "Validation loss decreased (1.077380 --> 1.076688).  Saving model ...\n",
      "Validation loss decreased (1.076688 --> 1.075997).  Saving model ...\n",
      "Validation loss decreased (1.075997 --> 1.075308).  Saving model ...\n",
      "Validation loss decreased (1.075308 --> 1.074620).  Saving model ...\n",
      "Validation loss decreased (1.074620 --> 1.073933).  Saving model ...\n",
      "Validation loss decreased (1.073933 --> 1.073248).  Saving model ...\n",
      "Validation loss decreased (1.073248 --> 1.072564).  Saving model ...\n",
      "Validation loss decreased (1.072564 --> 1.071882).  Saving model ...\n",
      "Validation loss decreased (1.071882 --> 1.071201).  Saving model ...\n",
      "Validation loss decreased (1.071201 --> 1.070521).  Saving model ...\n",
      "Validation loss decreased (1.070521 --> 1.069842).  Saving model ...\n",
      "Validation loss decreased (1.069842 --> 1.069165).  Saving model ...\n",
      "Validation loss decreased (1.069165 --> 1.068489).  Saving model ...\n",
      "Validation loss decreased (1.068489 --> 1.067815).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    Y_pred_test = net(X_test)\n",
    "    loss = loss_func(Y_pred_test, Y_test)\n",
    "    valid_losses.append(loss.item())\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    early_stoppings(valid_loss,net)\n",
    "    if early_stoppings.early_stop:\n",
    "        print('early stop')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot historical loss from `all_losses` during network learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix for training:\n",
      "[[30.  1.  4.  1.]\n",
      " [ 1. 36.  3.  0.]\n",
      " [ 2.  0. 30.  2.]\n",
      " [ 1.  0.  1. 37.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "confusion = torch.zeros(output_neurons, output_neurons)\n",
    "Y_pred = net(X)\n",
    "_, predicted = torch.max(F.softmax(Y_pred,1), 1)\n",
    "for i in range(x_array.shape[0]):\n",
    "    actual_class = Y.data[i]\n",
    "    predicted_class = predicted.data[i]\n",
    "    confusion[actual_class][predicted_class] += 1\n",
    "print('')\n",
    "print('Confusion matrix for training:')\n",
    "print(confusion.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array_test = test_data[:, 1:86]\n",
    "y_array_test = test_data[:, 0]\n",
    "X_test = torch.tensor(x_array_test, dtype=torch.float)\n",
    "Y_test = torch.tensor(y_array_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for training:\n",
      "[[11.  0.  1.  0.]\n",
      " [ 0.  7.  0.  1.]\n",
      " [ 1.  0. 12.  1.]\n",
      " [ 0.  0.  0.  9.]]\n",
      "Testing Accuracy: 90.70 %\n"
     ]
    }
   ],
   "source": [
    "Y_pred_test = net(X_test)\n",
    "_, predicted_test = torch.max(F.softmax(Y_pred_test,1), 1)\n",
    "total_test = predicted_test.size(0)\n",
    "correct_test = sum(predicted_test.data.numpy() == Y_test.data.numpy())\n",
    "confusion = torch.zeros(output_neurons, output_neurons)\n",
    "for j in range(x_array_test.shape[0]):\n",
    "    actual_class = Y_test.data[j]\n",
    "    predicted_class = predicted_test.data[j]\n",
    "    confusion[actual_class][predicted_class] += 1\n",
    "print('Confusion matrix for training:')\n",
    "print(confusion.numpy())    \n",
    "print('Testing Accuracy: %.2f %%' % (100 * correct_test / total_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP0 = confusion.numpy()[0][0]\n",
    "TP1 = confusion.numpy()[1][1]\n",
    "TP2 = confusion.numpy()[2][2]\n",
    "TP3 = confusion.numpy()[3][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP0 = confusion.numpy()[1][0]+confusion.numpy()[2][0]+confusion.numpy()[3][0]\n",
    "FP1 = confusion.numpy()[0][1]+confusion.numpy()[2][1]+confusion.numpy()[3][1]\n",
    "FP2 = confusion.numpy()[0][2]+confusion.numpy()[1][2]+confusion.numpy()[3][2]\n",
    "FP3 = confusion.numpy()[0][3]+confusion.numpy()[1][3]+confusion.numpy()[2][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN0 = confusion.numpy()[0][1]+confusion.numpy()[0][2]+confusion.numpy()[0][3]\n",
    "FN1 = confusion.numpy()[1][0]+confusion.numpy()[1][2]+confusion.numpy()[1][3]\n",
    "FN2 = confusion.numpy()[2][0]+confusion.numpy()[2][1]+confusion.numpy()[2][3]\n",
    "FN3 = confusion.numpy()[3][0]+confusion.numpy()[3][1]+confusion.numpy()[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166667 0.9166667 0.9166666865348816 1.0 0.875 0.9333333333333333 0.9230769 0.85714287 0.8888888955933893 0.8181818 1.0 0.9000000262260447\n"
     ]
    }
   ],
   "source": [
    "prcision0 = TP0/(TP0+FP0)\n",
    "prcision1 = TP1/(TP1+FP1)\n",
    "prcision2 = TP2/(TP2+FP2)\n",
    "prcision3 = TP3/(TP3+FP3)\n",
    "recall0 = TP0/(TP0+FN0)\n",
    "recall1 = TP1/(TP1+FN1)\n",
    "recall2 = TP2/(TP2+FN2)\n",
    "recall3 = TP3/(TP3+FN3)\n",
    "f1scor0 = 2*prcision0*recall0/(prcision0+recall0)\n",
    "f1scor1 = 2*prcision1*recall1/(prcision1+recall1)\n",
    "f1scor2 = 2*prcision2*recall2/(prcision2+recall2)\n",
    "f1scor3 = 2*prcision3*recall3/(prcision3+recall3)\n",
    "\n",
    "print(prcision0,recall0,f1scor0,prcision1,recall1,f1scor1,prcision2,recall2,f1scor2,prcision3,recall3,f1scor3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
